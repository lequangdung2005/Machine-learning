{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self,ids=None,children=[],entropy=0,depth=0):\n",
    "        self.ids=ids\n",
    "        self.entropy=entropy\n",
    "        self.depth=depth\n",
    "        self.split_attribute=None # chosen feature to divide into children\n",
    "        self.children=children\n",
    "        self.val_chosen=None      # val_chosen to save values for predictation\n",
    "        self.label=None\n",
    "\n",
    "    def set_val(self,split_attribute,val_chosen):\n",
    "        self.split_attribute=split_attribute\n",
    "        self.val_chosen=val_chosen    # function to save 2 val for predictation\n",
    "\n",
    "    def set_label(self,label):\n",
    "        self.label=label\n",
    "\n",
    "class decision_tree_id3(object):\n",
    "\n",
    "    def __init__(self,max_depth=10,min_sample_split=2,min_gain=1e-4):\n",
    "        self.root=None\n",
    "        self.max_depth=max_depth\n",
    "        self.min_sample_split=min_sample_split\n",
    "        self.Ntrain=0\n",
    "        self.min_gain=min_gain\n",
    "\n",
    "    def optimizer_step(self,data,target):\n",
    "        self.attributes = list(data)    # name features of data\n",
    "        self.Ntrain=data.count().iloc[0]   # number of data\n",
    "        self.data=data\n",
    "        self.target=target\n",
    "        self.labels=target.unique()     \n",
    "        ids=range(self.Ntrain)\n",
    "        self.root=Node(ids=ids,entropy=self.entropy(ids),depth=0)\n",
    "        queue=[self.root]\n",
    "        while queue:\n",
    "            node=queue.pop()\n",
    "            if node.depth<self.max_depth or node.entropy<self.min_gain:\n",
    "                node.children=self._split(node)\n",
    "                if not node.children:\n",
    "                    self._set_label(node)\n",
    "                queue+=node.children\n",
    "            else:\n",
    "                self._set_label(node) \n",
    "\n",
    "    def _split(self,node):\n",
    "        ids=node.ids\n",
    "        best_gain=0\n",
    "        best_splits=[]\n",
    "        best_attributes=None\n",
    "        val_chosen=None\n",
    "        \n",
    "        sub_data=self.data.iloc[ids,:] # sub data of this node\n",
    "        for i, att in enumerate(self.attributes):   \n",
    "            values=sub_data.iloc[:,i].unique().tolist()\n",
    "            if len(values)==1: continue\n",
    "            splits=[]\n",
    "            for value in values:\n",
    "                sub_ids=sub_data.index[sub_data[att]==value].tolist()\n",
    "                splits.append(sub_ids)\n",
    "            if min(map(len,splits))<self.min_sample_split : continue    # map function apply len function for each element of splits\n",
    "            entropy_=0\n",
    "            for split in splits:\n",
    "                entropy_+=len(split)*self.entropy(split)/len(ids)\n",
    "            gain=node.entropy-entropy_\n",
    "            if gain<self.min_gain: continue\n",
    "            if gain>best_gain:\n",
    "                best_gain=gain\n",
    "                best_attributes=att\n",
    "                best_splits=splits\n",
    "                val_chosen=values\n",
    "        node.set_val(best_attributes,val_chosen) \n",
    "        child_nodes=[Node(ids=split,entropy=self.entropy(split),depth=node.depth+1)for split in best_splits]\n",
    "        return child_nodes\n",
    "\n",
    "    def _set_label(self,node): \n",
    "        target_ids=node.ids\n",
    "        node.set_label(self.target.iloc[target_ids].mode().iloc[0])\n",
    "\n",
    "    def entropy(self,ids):\n",
    "        if len(ids) == 0: return 0\n",
    "        freq = np.array(self.target.iloc[ids].value_counts()) # the array with the number of the occurrences of each value                  \n",
    "        freq=freq[freq!=0]  #   delete 0 element(can't use in logarit function)\n",
    "        prob=freq/float(np.sum(freq))  \n",
    "        return -np.sum(prob*np.log(prob)) # apply entropy function\n",
    "    \n",
    "    def predict(self,new_data):\n",
    "        x=new_data\n",
    "        node=self.root\n",
    "        label=None\n",
    "        while node.children:    # recurive until reach leaf node\n",
    "            if x[node.split_attribute] not in node.val_chosen:\n",
    "                label='delete'         # prevent if the value of feature not in values of split_attribute since we use random sample\n",
    "                break\n",
    "            else:\n",
    "                node=node.children[node.val_chosen.index(x[node.split_attribute])]\n",
    "                label=node.label\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X,y):\n",
    "    m=X.shape[0]\n",
    "    ids=np.random.choice(m,m,replace=True)\n",
    "    return X.iloc[ids,:],y.iloc[ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_forest(object):\n",
    "    def __init__(self,n_tree=10,max_depth=3,min_sample_split=20):\n",
    "        self.n_tree=n_tree\n",
    "        self.max_depth=max_depth\n",
    "        self.min_sample_split=min_sample_split\n",
    "        self.trees=[]\n",
    "    def optimizer_step(self,X,y):\n",
    "        for i in range(self.n_tree):\n",
    "            tree=decision_tree_id3(max_depth=self.max_depth,min_sample_split=self.min_sample_split)\n",
    "            X_train,y_train=bootstrap(X,y)\n",
    "            tree.optimizer_step(X_train,y_train)\n",
    "            self.trees.append(tree)\n",
    "    def predict(self,X_test):\n",
    "        n_point=X_test.count().iloc[0]\n",
    "        labels=[None]*n_point\n",
    "        for i in range(n_point):\n",
    "            x=X_test.iloc[i,:]\n",
    "            pred=[tree.predict(x) for tree in self.trees]\n",
    "            label=mode(pred)\n",
    "            labels[i]=label\n",
    "        return labels\n",
    "def arrcuracy(model,X,y):\n",
    "    yhat=model.predict(X)\n",
    "    if 'delete' in yhat:\n",
    "        m=len(yhat.remove('delete'))\n",
    "    else:\n",
    "        m=len(yhat)\n",
    "    sum=np.sum(np.where(y==yhat,1,0))\n",
    "    return sum/m    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest=Random_forest(n_tree=20,max_depth=3)\n",
    "df=pd.read_csv('Breast_Cancer.csv')\n",
    "m=df.shape[0]\n",
    "X_train=df.iloc[:int(m*0.6),:-1]\n",
    "y_train=df.iloc[:int(m*0.6),-1]\n",
    "X_test=df.iloc[int(m*0.6):,:-1]\n",
    "y_test=df.iloc[int(m*0.6):,-1]\n",
    "forest.optimizer_step(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8503105590062112\n"
     ]
    }
   ],
   "source": [
    "print(arrcuracy(forest,X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
